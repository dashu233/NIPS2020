1. Logarithmic Pruning is All You Need

follow from 

Whatâ€™s hidden in a
randomly weighted neural network?(find an untrained sub-network achieve full trained network proformence)

Proving the lottery ticket hypothesis:
Pruning is all you need(2l-depth untrained sub-network can archieve l-depth trained network, by selecting neuron carefully. Theory)

Idea: use weight decomposition to obtain log(1/\epsilon) dependency on neurons for mimic weights

